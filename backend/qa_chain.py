import os
from langchain_community.vectorstores import FAISS
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain_together import ChatTogether
from langchain_core.messages import AIMessage
# These functions MUST be updated internally to use the new environment variable-based paths.
# (This was addressed in vector_store_golden_faiss.py, vector_store_scraped_faiss.py, and vector_store_video_faiss.py in previous rounds)
from backend.vector_store_golden_faiss import load_faiss_golden
from backend.vector_store_scraped_faiss import load_faiss_scraped
from backend.vector_store_video_faiss import load_faiss_video
from backend.web_search import search_tavily

def get_top_relevant_docs(vectorstore, query: str, k: int = 4, threshold: float = 1.5):
    """Return top documents above a similarity threshold (i.e., lower distance)."""
    results = vectorstore.similarity_search_with_score(query, k=k)

    print(f"Retrieved {len(results)} docs with scores:")
    for doc, score in results:
        print(f" - {score:.4f}")

    # â›” THIS LINE FILTERS OUT DOCUMENTS BASED ON THE THRESHOLD
    filtered_docs = [doc for doc, score in results if score < threshold]

    print(f"Filtered top docs (score < {threshold}): {len(filtered_docs)}")
    return filtered_docs

def get_answer(query: str) -> str:
    embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

    # 1ï¸âƒ£ Golden Data (Uploaded Files)
    try:
        # load_faiss_golden must be updated to correctly locate FAISS index within persistent_storage
        golden_db = load_faiss_golden(embeddings)
        golden_docs = get_top_relevant_docs(golden_db, query)
        if golden_docs:
            print("âœ… Answering from: ðŸ“„ Uploaded Documents")
            context = "\n\n".join([doc.page_content for doc in golden_docs])
            return ask_llm(query, context, source="ðŸ“„ Answer (from uploaded documents)")
    except Exception as e:
        print(f"[Golden Retrieval Error] {e}")

    # 2ï¸âƒ£ Scraped Data
    try:
        # load_faiss_scraped must be updated to correctly locate FAISS index within persistent_storage
        scraped_db = load_faiss_scraped(embeddings)
        scraped_docs = get_top_relevant_docs(scraped_db, query)
        if scraped_docs:
            print("âœ… Answering from: ðŸŒ Scraped Websites")
            context = "\n\n".join([doc.page_content for doc in scraped_docs])
            return ask_llm(query, context, source="ðŸŒ Answer (from scraped websites)")
    except Exception as e:
        print(f"[Scraped Retrieval Error] {e}")

    # 3ï¸âƒ£ Video Data
    try:
        # load_faiss_video must be updated to correctly locate FAISS index within persistent_storage
        video_db = load_faiss_video(embeddings)
        video_docs = get_top_relevant_docs(video_db, query)
        if video_docs:
            print("âœ… Answering from: ðŸŽ¬ Video Transcripts")
            context = "\n\n".join([doc.page_content for doc in video_docs])
            return ask_llm(query, context, source="ðŸŽ¬ Answer (from ingested videos)")
    except Exception as e:
        print(f"[Video Retrieval Error] {e}")

    # 4ï¸âƒ£ Tavily Internet Search
    try:
        print("ðŸ” Falling back to Internet (Tavily)...")
        tavily_answer = search_tavily(query)
        return f"ðŸŒ **Answer(via Internet) :**\n{tavily_answer}"
    except Exception as e:
        return f"âŒ Internet fallback failed: {e}"

def ask_llm(query: str, context: str, source: str) -> str:
    prompt = f"""You are an expert assistant. Use the context to answer concisely.

Context:
{context}

Question: {query}
Answer:"""

    llm = ChatTogether(model="mistralai/Mistral-7B-Instruct-v0.2", temperature=0.4)
    response = llm.invoke(prompt)

    if isinstance(response, AIMessage):
        answer_text = response.content.strip()
    else:
        answer_text = str(response).strip()

    return f"{source}:\n{answer_text}"